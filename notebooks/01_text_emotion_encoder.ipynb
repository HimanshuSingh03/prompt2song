{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1079e1",
   "metadata": {},
   "source": [
    "# Prompt2Song – Text Emotion Encoder\n",
    "\n",
    "Train an emotion-aware text encoder that powers both prompt understanding and lyric embeddings. This notebook ingests the six-class emotion dataset, fine-tunes a lightweight transformer, and exports reusable helpers for downstream stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd761f9",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Inspect the prompt emotion dataset and build consistent label mappings\n",
    "- Fine-tune a DistilBERT-sized encoder on the six emotion classes\n",
    "- Export utility functions for prompt/lyric embeddings and persist artifacts for later notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb7e5",
   "metadata": {},
   "source": [
    "> ⚙️ **Offline-friendly setup**  \n",
    "Ensure the base model weights (e.g. `distilbert-base-uncased`) are cached locally before running. Set `HF_HOME` or `TRANSFORMERS_CACHE` if you maintain an offline cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7321bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umarkhan/.pyenv/versions/3.12.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoConfig,\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers before running this notebook.\") from exc\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install scikit-learn before running this notebook.\") from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf65a1",
   "metadata": {},
   "source": [
    "Here we resolve project directories, ensure artifact folders exist, and define constants such as the dataset roots, model name, and random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0abca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/umarkhan/repos/school/prompt2song\n",
      "Saving artifacts to: /Users/umarkhan/repos/school/prompt2song/artifacts/text_encoder\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "if (NOTEBOOK_DIR / \"datasets\").exists():\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "DATASET_ROOT = PROJECT_ROOT / \"datasets\" / \"emotions_NLP\"\n",
    "ARTIFACT_ROOT = PROJECT_ROOT / \"artifacts\" / \"text_encoder\"\n",
    "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "SEED = 13\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Saving artifacts to: {ARTIFACT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf047e",
   "metadata": {},
   "source": [
    "Utility helper that seeds NumPy and PyTorch for reproducibility, then applies it using the configured seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6803fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 13) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b097f",
   "metadata": {},
   "source": [
    "Loads the train/val/test splits from disk, cleans whitespace, filters empty rows, and prints a small preview plus dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8510392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    label\n",
      "0                            i didnt feel humiliated  sadness\n",
      "1  i can go from feeling so hopeless to so damned...  sadness\n",
      "2   im grabbing a minute to post i feel greedy wrong    anger\n",
      "3  i am ever feeling nostalgic about the fireplac...     love\n",
      "4                               i am feeling grouchy    anger\n",
      "{'train': 16000, 'val': 2000, 'test': 2000}\n"
     ]
    }
   ],
   "source": [
    "def load_split(split: str) -> pd.DataFrame:\n",
    "    path = DATASET_ROOT / f\"{split}.txt\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_csv(path, sep=\";\", names=[\"text\", \"label\"], encoding=\"utf-8\")\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df = load_split(\"val\")\n",
    "test_df = load_split(\"test\")\n",
    "\n",
    "print(train_df.head())\n",
    "print({split: len(df) for split, df in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325989d",
   "metadata": {},
   "source": [
    "Builds label↔id lookup tables, attaches numeric labels to each split, and reports label distribution for sanity checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60ef21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'anger': 0, 'fear': 1, 'joy': 2, 'love': 3, 'sadness': 4, 'surprise': 5}\n",
      "Train distribution: label\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n",
      "Val distribution: label\n",
      "joy         704\n",
      "sadness     550\n",
      "anger       275\n",
      "fear        212\n",
      "love        178\n",
      "surprise     81\n",
      "Name: count, dtype: int64\n",
      "Test distribution: label\n",
      "joy         695\n",
      "sadness     581\n",
      "anger       275\n",
      "fear        224\n",
      "love        159\n",
      "surprise     66\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(train_df[\"label\"].unique())\n",
    "labels_val = sorted(val_df[\"label\"].unique())\n",
    "labels_test = sorted(test_df[\"label\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "label2id_val = {label: idx for idx, label in enumerate(labels_val)}\n",
    "id2label_val = {idx: label for label, idx in label2id_val.items()}\n",
    "\n",
    "label2id_test = {label: idx for idx, label in enumerate(labels_test)}\n",
    "id2label_test = {idx: label for label, idx in label2id_test.items()}\n",
    "\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "print(\"Label mapping:\", label2id)\n",
    "print(\"Train distribution:\", train_df[\"label\"].value_counts())\n",
    "print(\"Val distribution:\", val_df[\"label\"].value_counts())\n",
    "print(\"Test distribution:\", test_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789a61d",
   "metadata": {},
   "source": [
    "Initializes the tokenizer and defines an EmotionDataset wrapper that tokenizes text samples on the fly before instantiating dataset objects for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c59402",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 128):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            row[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoded.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label_id\"], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_df, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a48bef",
   "metadata": {},
   "source": [
    "Creates a classification configuration for DistilBERT and loads the base model with the updated label metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9657b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06f0d4",
   "metadata": {},
   "source": [
    "Defines compute_metrics so the Trainer can report accuracy and macro/weighted F1 scores during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfeaf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdb68f",
   "metadata": {},
   "source": [
    "### Trainer configuration\n",
    "Adjust `per_device_train_batch_size`, `num_train_epochs`, or `learning_rate` for your hardware budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85733489",
   "metadata": {},
   "source": [
    "Configures TrainingArguments (batch sizes, epochs, evaluation cadence, etc.) and instantiates the Hugging Face Trainer with datasets and metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c927ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ARTIFACT_ROOT / \"checkpoints\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41792f06",
   "metadata": {},
   "source": [
    "Optionally kicks off fine-tuning via trainer.train() and evaluates the pre-trained checkpoint to provide baseline metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3982ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umarkhan/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 08:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.208100</td>\n",
       "      <td>0.205605</td>\n",
       "      <td>0.930500</td>\n",
       "      <td>0.905588</td>\n",
       "      <td>0.930564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.908519</td>\n",
       "      <td>0.935179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.156752</td>\n",
       "      <td>0.940500</td>\n",
       "      <td>0.920083</td>\n",
       "      <td>0.940425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umarkhan/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/umarkhan/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/umarkhan/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15675245225429535, 'eval_accuracy': 0.9405, 'eval_f1_macro': 0.9200826669819583, 'eval_f1_weighted': 0.9404253672398842, 'eval_runtime': 6.0529, 'eval_samples_per_second': 330.418, 'eval_steps_per_second': 10.408, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to launch full fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Example: evaluate initial (pre-trained) checkpoint before fine-tuning\n",
    "pretrain_metrics = trainer.evaluate()\n",
    "print(pretrain_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8d317",
   "metadata": {},
   "source": [
    "Runs final evaluation on the test split, then saves the fine-tuned model, tokenizer, and label mapping artifacts for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833ba7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umarkhan/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'eval_loss': 0.1675693839788437, 'eval_accuracy': 0.9305, 'eval_f1_macro': 0.8889008013155412, 'eval_f1_weighted': 0.9300367302930868, 'eval_runtime': 6.4241, 'eval_samples_per_second': 311.327, 'eval_steps_per_second': 9.807, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# After calling trainer.train(), run evaluation and save artifacts\n",
    "eval_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test metrics:\", eval_metrics)\n",
    "\n",
    "trainer.save_model(ARTIFACT_ROOT / \"hf_model\")\n",
    "tokenizer.save_pretrained(ARTIFACT_ROOT / \"hf_model\")\n",
    "\n",
    "with open(ARTIFACT_ROOT / \"label2id.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(label2id, fp, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02321df8",
   "metadata": {},
   "source": [
    "## Embedding helper\n",
    "Use the fine-tuned checkpoint to embed both prompts and lyrics via mean pooling of token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa61da7",
   "metadata": {},
   "source": [
    "Implements a lightweight TextEmotionEncoder wrapper that loads the fine-tuned model and exposes a mean-pooled encode() helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d73f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmotionEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_dir: Path, device: str | None = None):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModel\n",
    "\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_model = AutoModel.from_pretrained(model_dir).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch = texts[start:start+batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            outputs = self.base_model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            attention_mask = tokens.attention_mask.unsqueeze(-1)\n",
    "            summed = (token_embeddings * attention_mask).sum(dim=1)\n",
    "            counts = attention_mask.sum(dim=1)\n",
    "            mean_pooled = summed / counts\n",
    "            embeddings.append(mean_pooled.cpu().numpy())\n",
    "        return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f44614",
   "metadata": {},
   "source": [
    "Demonstrates encoding the training prompts with the helper and persists the resulting embeddings for downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6756feec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage (requires fine-tuned weights saved above)\u001b[39;00m\n\u001b[32m      2\u001b[39m encoder = TextEmotionEncoder(ARTIFACT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mhf_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m prompt_embeddings = \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m np.save(ARTIFACT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mtrain_prompt_embeddings.npy\u001b[39m\u001b[33m\"\u001b[39m, prompt_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mTextEmotionEncoder.encode\u001b[39m\u001b[34m(self, texts, batch_size)\u001b[39m\n\u001b[32m     14\u001b[39m batch = texts[start:start+batch_size]\n\u001b[32m     15\u001b[39m tokens = \u001b[38;5;28mself\u001b[39m.tokenizer(\n\u001b[32m     16\u001b[39m     batch,\n\u001b[32m     17\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m ).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m token_embeddings = outputs.last_hidden_state\n\u001b[32m     24\u001b[39m attention_mask = tokens.attention_mask.unsqueeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:484\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    481\u001b[39m sa_output = \u001b[38;5;28mself\u001b[39m.sa_layer_norm(sa_output + x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m ffn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    485\u001b[39m ffn_output: torch.Tensor = \u001b[38;5;28mself\u001b[39m.output_layer_norm(ffn_output + sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    487\u001b[39m output = (ffn_output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:418\u001b[39m, in \u001b[36mFFN.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:422\u001b[39m, in \u001b[36mFFN.ff_chunk\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m    421\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.lin1(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.lin2(x)\n\u001b[32m    424\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.12/lib/python3.12/site-packages/transformers/activations.py:85\u001b[39m, in \u001b[36mGELUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage (requires fine-tuned weights saved above)\n",
    "encoder = TextEmotionEncoder(ARTIFACT_ROOT / \"hf_model\")\n",
    "prompt_embeddings = encoder.encode(train_df[\"text\"].tolist())\n",
    "np.save(ARTIFACT_ROOT / \"train_prompt_embeddings.npy\", prompt_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850f64c",
   "metadata": {},
   "source": [
    "#### Quick demo\n",
    "Use a couple of sample prompts to verify the saved encoder produces fixed-size embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed66b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1, 768)\n",
      "Sample embeddings: -0.54034615\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test for the text encoder\n",
    "if (ARTIFACT_ROOT / 'hf_model').exists():\n",
    "    demo_encoder = TextEmotionEncoder(ARTIFACT_ROOT / 'hf_model')\n",
    "    sample_prompts = [\n",
    "        'I feel hopeful and excited about tomorrow',\n",
    "    ]\n",
    "    demo_embeddings = demo_encoder.encode(sample_prompts)\n",
    "    print('Embeddings shape:', demo_embeddings.shape)\n",
    "    print('Sample embeddings:', demo_embeddings[0][2])\n",
    "else:\n",
    "    print('⚠️ Fine-tuned weights not found; run trainer.train() and save artifacts first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e314991",
   "metadata": {},
   "source": [
    "## Build merged Spotify lyrics dataset\n",
    "Combine `songs.csv` and `spotify_songs.csv`, normalize key columns, and persist a cleaned dataset we can reuse for model training or lyric retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b47e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged songs: 18,247 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_release_date</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>playlist_subgenre</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>lyrics_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>4cZ3UsiKd1kUQIaq4BFIj2</td>\n",
       "      <td>no one compares to you</td>\n",
       "      <td>Jack&amp;jack</td>\n",
       "      <td></td>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64183</td>\n",
       "      <td>0.742462</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078705</td>\n",
       "      <td>0.635036</td>\n",
       "      <td>0.346527</td>\n",
       "      <td>182920</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>thoughts of you and me keep passing by like sh...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>songs_csv</td>\n",
       "      <td>2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5589</th>\n",
       "      <td>25YvGDl2zSE0pH8jrMZ6aY</td>\n",
       "      <td>Therapy</td>\n",
       "      <td>Khalid</td>\n",
       "      <td>American Teen</td>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>urban contemporary</td>\n",
       "      <td>0.70700</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00344</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>92.988000</td>\n",
       "      <td>257960</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>something that you're doing has me falling all...</td>\n",
       "      <td>en</td>\n",
       "      <td>spotify_songs_csv</td>\n",
       "      <td>1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14580</th>\n",
       "      <td>6aDYL0klsShJ1NDScruk2G</td>\n",
       "      <td>Stars</td>\n",
       "      <td>Callalily</td>\n",
       "      <td>Destination XYZ</td>\n",
       "      <td>2006-07-17</td>\n",
       "      <td>rock</td>\n",
       "      <td>classic rock</td>\n",
       "      <td>0.50100</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>126.207000</td>\n",
       "      <td>233307</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>a picture of you reminds me how the years have...</td>\n",
       "      <td>en</td>\n",
       "      <td>spotify_songs_csv</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      song_id                    name    artists  \\\n",
       "217    4cZ3UsiKd1kUQIaq4BFIj2  no one compares to you  Jack&jack   \n",
       "5589   25YvGDl2zSE0pH8jrMZ6aY                 Therapy     Khalid   \n",
       "14580  6aDYL0klsShJ1NDScruk2G                   Stars  Callalily   \n",
       "\n",
       "            album_name album_release_date playlist_genre   playlist_subgenre  \\\n",
       "217                            2019-01-25            Pop                 NaN   \n",
       "5589     American Teen         2017-04-27            r&b  urban contemporary   \n",
       "14580  Destination XYZ         2006-07-17           rock        classic rock   \n",
       "\n",
       "       danceability    energy       key  ...  instrumentalness  liveness  \\\n",
       "217         0.64183  0.742462  0.090909  ...               NaN  0.078705   \n",
       "5589        0.70700  0.484000  9.000000  ...           0.00344  0.256000   \n",
       "14580       0.50100  0.555000  7.000000  ...           0.00003  0.085900   \n",
       "\n",
       "        valence       tempo  duration_ms  track_popularity  \\\n",
       "217    0.635036    0.346527       182920          0.136364   \n",
       "5589   0.430000   92.988000       257960         65.000000   \n",
       "14580  0.436000  126.207000       233307         60.000000   \n",
       "\n",
       "                                                  lyrics  language  \\\n",
       "217    thoughts of you and me keep passing by like sh...   unknown   \n",
       "5589   something that you're doing has me falling all...        en   \n",
       "14580  a picture of you reminds me how the years have...        en   \n",
       "\n",
       "                  source  lyrics_length  \n",
       "217            songs_csv           2969  \n",
       "5589   spotify_songs_csv           1642  \n",
       "14580  spotify_songs_csv           1214  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "SONGS_META_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"songs.csv\"\n",
    "SPOTIFY_SONGS_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"spotify_songs.csv\"\n",
    "COMBINED_SONGS_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"merged_spotify_songs.csv\"\n",
    "\n",
    "CANONICAL_COLUMNS = [\n",
    "    \"song_id\",\n",
    "    \"name\",\n",
    "    \"artists\",\n",
    "    \"album_name\",\n",
    "    \"album_release_date\",\n",
    "    \"playlist_genre\",\n",
    "    \"playlist_subgenre\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"duration_ms\",\n",
    "    \"track_popularity\",\n",
    "    \"lyrics\",\n",
    "    \"language\",\n",
    "    \"source\",\n",
    "]\n",
    "\n",
    "def _parse_lyrics(raw: str) -> str:\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    text = str(raw).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "        try:\n",
    "            tokens = ast.literal_eval(text)\n",
    "            if isinstance(tokens, (list, tuple)):\n",
    "                text = \" \".join(str(tok) for tok in tokens)\n",
    "        except Exception:\n",
    "            text = text.replace(\"[\", \" \").replace(\"]\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def _clean_artists(value: str) -> str:\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    value = str(value)\n",
    "    value = value.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    value = value.replace(\"'\", \"\")\n",
    "    value = re.sub(r\"\\s+\", \" \", value)\n",
    "    return value.strip()\n",
    "\n",
    "def _standardize_song_frame(df: pd.DataFrame, source: str) -> pd.DataFrame:\n",
    "    rename_map = {\n",
    "        \"track_id\": \"song_id\",\n",
    "        \"track_name\": \"name\",\n",
    "        \"track_artist\": \"artists\",\n",
    "        \"track_album_name\": \"album_name\",\n",
    "        \"track_album_release_date\": \"album_release_date\",\n",
    "        \"playlist_subgenre\": \"playlist_subgenre\",\n",
    "        \"language\": \"language\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "    keep_cols = [col for col in CANONICAL_COLUMNS if col in df.columns]\n",
    "    missing_cols = [col for col in CANONICAL_COLUMNS if col not in df.columns]\n",
    "    for col in missing_cols:\n",
    "        df[col] = np.nan\n",
    "    df = df[CANONICAL_COLUMNS].copy()\n",
    "    df[\"source\"] = source\n",
    "    df[\"lyrics\"] = df[\"lyrics\"].apply(_parse_lyrics)\n",
    "    df = df[df[\"lyrics\"].str.len() >= 30]\n",
    "    df[\"artists\"] = df[\"artists\"].apply(_clean_artists)\n",
    "    df[\"album_name\"] = df[\"album_name\"].fillna(\"\")\n",
    "    df[\"language\"] = df[\"language\"].fillna(\"unknown\")\n",
    "    numeric_cols = [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"key\",\n",
    "        \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "        \"duration_ms\",\n",
    "        \"track_popularity\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"song_id\", \"lyrics\"])\n",
    "    df[\"lyrics\"] = df[\"lyrics\"].str.lower()\n",
    "    df[\"lyrics\"] = df[\"lyrics\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df[\"lyrics_length\"] = df[\"lyrics\"].str.len()\n",
    "    return df\n",
    "\n",
    "songs_base = pd.read_csv(SONGS_META_PATH)\n",
    "spotify_additional = pd.read_csv(SPOTIFY_SONGS_PATH)\n",
    "\n",
    "songs_base = _standardize_song_frame(songs_base, source=\"songs_csv\")\n",
    "spotify_additional = _standardize_song_frame(spotify_additional, source=\"spotify_songs_csv\")\n",
    "\n",
    "merged_songs = pd.concat([songs_base, spotify_additional], ignore_index=True)\n",
    "merged_songs = merged_songs.drop_duplicates(subset=[\"song_id\"])\n",
    "merged_songs = merged_songs.reset_index(drop=True)\n",
    "\n",
    "print(f\"Merged songs: {len(merged_songs):,} rows\")\n",
    "display(merged_songs.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7b52c",
   "metadata": {},
   "source": [
    "Persist the cleaned corpus for downstream training/reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76986bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged dataset to /Users/umarkhan/repos/school/prompt2song/datasets/song_features/merged_spotify_songs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>album_release_date</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>playlist_subgenre</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>lyrics_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6oJ6le65B3SEqPwMRNXWjY</td>\n",
       "      <td>higher love</td>\n",
       "      <td>Kygo</td>\n",
       "      <td></td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632680</td>\n",
       "      <td>0.667346</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086004</td>\n",
       "      <td>0.391370</td>\n",
       "      <td>0.290605</td>\n",
       "      <td>228267</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>bring me higher love, love bring me higher lov...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>songs_csv</td>\n",
       "      <td>1683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3yNZ5r3LKfdmjoS3gkhUCT</td>\n",
       "      <td>bad guy (with justin bieber)</td>\n",
       "      <td>Billieeilish</td>\n",
       "      <td></td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>Pop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.602614</td>\n",
       "      <td>0.425904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.102930</td>\n",
       "      <td>0.687634</td>\n",
       "      <td>0.508374</td>\n",
       "      <td>194840</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>yeah, yeah oh, ah white shirt now red, my bloo...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>songs_csv</td>\n",
       "      <td>5531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  song_id                          name       artists  \\\n",
       "0  6oJ6le65B3SEqPwMRNXWjY                   higher love          Kygo   \n",
       "1  3yNZ5r3LKfdmjoS3gkhUCT  bad guy (with justin bieber)  Billieeilish   \n",
       "\n",
       "  album_name album_release_date playlist_genre playlist_subgenre  \\\n",
       "0                    2019-06-28            Pop               NaN   \n",
       "1                    2019-07-11            Pop               NaN   \n",
       "\n",
       "   danceability    energy       key  ...  instrumentalness  liveness  \\\n",
       "0      0.632680  0.667346  0.727273  ...               NaN  0.086004   \n",
       "1      0.602614  0.425904  0.000000  ...               NaN  0.102930   \n",
       "\n",
       "    valence     tempo  duration_ms  track_popularity  \\\n",
       "0  0.391370  0.290605       228267          0.500000   \n",
       "1  0.687634  0.508374       194840          0.318182   \n",
       "\n",
       "                                              lyrics  language     source  \\\n",
       "0  bring me higher love, love bring me higher lov...   unknown  songs_csv   \n",
       "1  yeah, yeah oh, ah white shirt now red, my bloo...   unknown  songs_csv   \n",
       "\n",
       "   lyrics_length  \n",
       "0           1683  \n",
       "1           5531  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_songs.to_csv(COMBINED_SONGS_PATH, index=False)\n",
    "print(f\"Saved merged dataset to {COMBINED_SONGS_PATH}\")\n",
    "merged_songs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c96a82",
   "metadata": {},
   "source": [
    "## Lyrics emotion tagging & retrieval setup\n",
    "Sample ~1k songs with lyrics, clean the text, and prep artifacts so prompts and lyrics share the same embedding space for quick experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d6d66c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1000 songs from 18247 lyric rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6KQxGfOdjHMh5wT15t0gJj</td>\n",
       "      <td>All Into Nothing</td>\n",
       "      <td>R3HAB</td>\n",
       "      <td>All Into Nothing</td>\n",
       "      <td>haven't seen you since the summer strangers ta...</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4e9eGQYsOiBcftrWXwsVco</td>\n",
       "      <td>Aerials</td>\n",
       "      <td>System Of A Down</td>\n",
       "      <td>Toxicity</td>\n",
       "      <td>life is a waterfall we're one in the river and...</td>\n",
       "      <td>1078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2TQaMcc3BEAF5Srn7FCipl</td>\n",
       "      <td>Always</td>\n",
       "      <td>Atlantic Starr</td>\n",
       "      <td>All In The Name Of Love</td>\n",
       "      <td>girl, you are to me all that a woman should be...</td>\n",
       "      <td>693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FigU3fBZd8gtBYR3XUx5e</td>\n",
       "      <td>Fade All My Life</td>\n",
       "      <td>Unlike Pluto</td>\n",
       "      <td>Fade All My Life</td>\n",
       "      <td>i don't wanna know how how to read my own mess...</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6oGPZcErzd9Hjw3E4sDBrq</td>\n",
       "      <td>If It Isn't Love</td>\n",
       "      <td>New Edition</td>\n",
       "      <td>Greatest Hits-Volume One</td>\n",
       "      <td>i don't love her i tried to tell myself but yo...</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  song_id              name           artists  \\\n",
       "0  6KQxGfOdjHMh5wT15t0gJj  All Into Nothing             R3HAB   \n",
       "1  4e9eGQYsOiBcftrWXwsVco           Aerials  System Of A Down   \n",
       "2  2TQaMcc3BEAF5Srn7FCipl            Always    Atlantic Starr   \n",
       "3  0FigU3fBZd8gtBYR3XUx5e  Fade All My Life      Unlike Pluto   \n",
       "4  6oGPZcErzd9Hjw3E4sDBrq  If It Isn't Love       New Edition   \n",
       "\n",
       "                 album_name  \\\n",
       "0          All Into Nothing   \n",
       "1                  Toxicity   \n",
       "2   All In The Name Of Love   \n",
       "3          Fade All My Life   \n",
       "4  Greatest Hits-Volume One   \n",
       "\n",
       "                                              lyrics  lyrics_length  \n",
       "0  haven't seen you since the summer strangers ta...            917  \n",
       "1  life is a waterfall we're one in the river and...           1078  \n",
       "2  girl, you are to me all that a woman should be...            693  \n",
       "3  i don't wanna know how how to read my own mess...           1046  \n",
       "4  i don't love her i tried to tell myself but yo...           1955  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LYRICS_SAMPLE_SIZE = 1000\n",
    "SONG_DATA_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"merged_spotify_songs.csv\"\n",
    "LYRICS_ARTIFACT_DIR = ARTIFACT_ROOT / \"lyrics_retrieval\"\n",
    "LYRICS_ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_lyrics_sample(csv_path: Path, sample_size: int = 1000, chunk_size: int = 5000) -> pd.DataFrame:\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    reservoir: List[dict] = []\n",
    "    total_rows = 0\n",
    "    usecols = [\"song_id\", \"name\", \"album_name\", \"artists\", \"lyrics\"]\n",
    "\n",
    "    for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_size):\n",
    "        chunk = chunk.dropna(subset=[\"lyrics\"])\n",
    "        chunk[\"lyrics\"] = (\n",
    "            chunk[\"lyrics\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        chunk = chunk[chunk[\"lyrics\"].str.len() >= 30]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        for row in chunk.itertuples(index=False):\n",
    "            total_rows += 1\n",
    "            row_dict = row._asdict()\n",
    "            if len(reservoir) < sample_size:\n",
    "                reservoir.append(row_dict)\n",
    "            else:\n",
    "                j = rng.integers(0, total_rows)\n",
    "                if j < sample_size:\n",
    "                    reservoir[j] = row_dict\n",
    "\n",
    "    if not reservoir:\n",
    "        raise ValueError(\"No lyric rows sampled; check the dataset path.\")\n",
    "\n",
    "    sampled = pd.DataFrame(reservoir).reset_index(drop=True)\n",
    "    sampled[\"lyrics_length\"] = sampled[\"lyrics\"].str.len()\n",
    "    print(f\"Collected {len(sampled)} songs from {total_rows} lyric rows.\")\n",
    "    return sampled\n",
    "\n",
    "lyrics_df = load_lyrics_sample(SONG_DATA_PATH, sample_size=LYRICS_SAMPLE_SIZE)\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec892d",
   "metadata": {},
   "source": [
    "Predict an emotion label for each lyric with the fine-tuned classifier, embed the lyrics with the shared encoder, and persist lightweight artifacts for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a47c9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lyric embeddings to /Users/umarkhan/repos/school/prompt2song/artifacts/text_encoder/lyrics_retrieval/lyrics_embeddings.npy\n",
      "Saved lyric metadata to /Users/umarkhan/repos/school/prompt2song/artifacts/text_encoder/lyrics_retrieval/lyrics_metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "if not (ARTIFACT_ROOT / \"hf_model\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing fine-tuned checkpoint at artifacts/text_encoder/hf_model. Run trainer.train() and rerun the save cell first.\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ARTIFACT_ROOT / \"hf_model\"\n",
    ").to(device)\n",
    "emotion_classifier.eval()\n",
    "\n",
    "lyrics_encoder = TextEmotionEncoder(ARTIFACT_ROOT / \"hf_model\", device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_emotions(texts: List[str], batch_size: int = 32) -> List[str]:\n",
    "    labels = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        encodings = lyrics_encoder.tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        logits = emotion_classifier(**encodings).logits\n",
    "        labels.extend(logits.argmax(dim=-1).cpu().tolist())\n",
    "    return [id2label[idx] for idx in labels]\n",
    "\n",
    "lyrics_df[\"emotion_label\"] = predict_emotions(lyrics_df[\"lyrics\"].tolist())\n",
    "\n",
    "lyrics_embeddings = lyrics_encoder.encode(lyrics_df[\"lyrics\"].tolist(), batch_size=32)\n",
    "lyric_unit_embeddings = lyrics_embeddings / np.clip(\n",
    "    np.linalg.norm(lyrics_embeddings, axis=1, keepdims=True),\n",
    "    a_min=1e-12,\n",
    "    a_max=None,\n",
    ")\n",
    "\n",
    "np.save(LYRICS_ARTIFACT_DIR / \"lyrics_embeddings.npy\", lyrics_embeddings)\n",
    "\n",
    "metadata_cols = [\n",
    "    \"song_id\",\n",
    "    \"name\",\n",
    "    \"album_name\",\n",
    "    \"artists\",\n",
    "    \"emotion_label\",\n",
    "    \"lyrics\",\n",
    "    \"lyrics_length\",\n",
    "]\n",
    "lyrics_metadata_path = LYRICS_ARTIFACT_DIR / \"lyrics_metadata.jsonl\"\n",
    "lyrics_df[metadata_cols].to_json(\n",
    "    lyrics_metadata_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "\n",
    "print(f\"Saved lyric embeddings to {LYRICS_ARTIFACT_DIR / 'lyrics_embeddings.npy'}\")\n",
    "print(f\"Saved lyric metadata to {lyrics_metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c7718",
   "metadata": {},
   "source": [
    "Retrieve the top-N songs whose lyric embeddings are closest to a prompt embedding (cosine similarity in the shared space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "146b3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Needed Me — Rihanna | sadness | score=0.941\n",
      "mustard on the beat, ho! i was good on my own, that's the way it was that's the way it was you was good on the low for a faded fuck on some faded love shit, what the fuck you complaining for? feeling ...\n",
      "--------------------------------------------------------------------------------\n",
      "2. The Truth — Sevendust | sadness | score=0.939\n",
      "no more time for saving what the hell did you give? what the hell did you do? just more numb to waste it you will mean nothing you will be nothing one more tragic instant lost right underneath while y...\n",
      "--------------------------------------------------------------------------------\n",
      "3. Wasted On You — Louis Futon | sadness | score=0.934\n",
      "do you only love me when you're high? will you still want me when you're not? i've got this poison running deep in my mind, my mind cause i think that i'm the only one but maybe you just know what i w...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def retrieve_songs(prompt: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    if lyrics_df.empty:\n",
    "        raise ValueError(\"lyrics_df is empty. Run the sampling cell first.\")\n",
    "\n",
    "    prompt_vec = lyrics_encoder.encode([prompt])[0]\n",
    "    prompt_vec = prompt_vec / max(np.linalg.norm(prompt_vec), 1e-12)\n",
    "    scores = lyric_unit_embeddings @ prompt_vec\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    results = lyrics_df.iloc[top_idx].copy()\n",
    "    results[\"similarity\"] = scores[top_idx]\n",
    "    return results[[\"name\", \"artists\", \"album_name\", \"emotion_label\", \"similarity\", \"lyrics\"]]\n",
    "\n",
    "demo_prompt = \"I just spilt water on my notes and ruined all my hardwork\"\n",
    "top_matches = retrieve_songs(demo_prompt, top_k=3)\n",
    "\n",
    "for rank, row in enumerate(top_matches.itertuples(index=False), start=1):\n",
    "    print(f\"{rank}. {row.name} — {row.artists} | {row.emotion_label} | score={row.similarity:.3f}\")\n",
    "    print(row.lyrics[:200].replace(\"\\n\", \" \") + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec26490",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Run `trainer.train()` and persist the fine-tuned checkpoint once satisfied with metrics.\n",
    "- Use `TextEmotionEncoder` to embed the prompt dataset (`train`, `val`, `test`) for analysis or curricula.\n",
    "- Proceed to the audio encoder and multimodal fusion notebook after exporting `artifacts/text_encoder/hf_model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
