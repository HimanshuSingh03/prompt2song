{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1079e1",
   "metadata": {},
   "source": [
    "# Prompt2Song – Text Emotion Encoder\n",
    "\n",
    "Train an emotion-aware text encoder that powers both prompt understanding and lyric embeddings. This notebook ingests the six-class emotion dataset, fine-tunes a lightweight transformer, and exports reusable helpers for downstream stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd761f9",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Inspect the prompt emotion dataset and build consistent label mappings\n",
    "- Fine-tune a DistilBERT-sized encoder on the six emotion classes\n",
    "- Export utility functions for prompt/lyric embeddings and persist artifacts for later notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb7e5",
   "metadata": {},
   "source": [
    "> ⚙️ **Offline-friendly setup**  \n",
    "Ensure the base model weights (e.g. `distilbert-base-uncased`) are cached locally before running. Set `HF_HOME` or `TRANSFORMERS_CACHE` if you maintain an offline cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7321bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoConfig,\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        EarlyStoppingCallback,\n",
    "    )\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers before running this notebook.\") from exc\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install scikit-learn before running this notebook.\") from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf65a1",
   "metadata": {},
   "source": [
    "Here we resolve project directories, ensure artifact folders exist, and define constants such as the dataset roots, model name, and random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0abca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "if (NOTEBOOK_DIR / \"datasets\").exists():\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "DATASET_ROOT = PROJECT_ROOT / \"datasets\" / \"emotions_NLP\"\n",
    "ARTIFACT_ROOT = PROJECT_ROOT / \"artifacts\" / \"text_encoder\"\n",
    "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "SEED = 13\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Saving artifacts to: {ARTIFACT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf047e",
   "metadata": {},
   "source": [
    "Utility helper that seeds NumPy and PyTorch for reproducibility, then applies it using the configured seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 13) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b097f",
   "metadata": {},
   "source": [
    "Loads the train/val/test splits from disk, cleans whitespace, filters empty rows, and prints a small preview plus dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8510392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split: str) -> pd.DataFrame:\n",
    "    path = DATASET_ROOT / f\"{split}.txt\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_csv(path, sep=\";\", names=[\"text\", \"label\"], encoding=\"utf-8\")\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_df = load_split(\"train\")\n",
    "val_df = load_split(\"val\")\n",
    "test_df = load_split(\"test\")\n",
    "\n",
    "print(train_df.head())\n",
    "print({split: len(df) for split, df in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e325989d",
   "metadata": {},
   "source": [
    "Builds label↔id lookup tables, attaches numeric labels to each split, and reports label distribution for sanity checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(train_df[\"label\"].unique())\n",
    "labels_val = sorted(val_df[\"label\"].unique())\n",
    "labels_test = sorted(test_df[\"label\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "label2id_val = {label: idx for idx, label in enumerate(labels_val)}\n",
    "id2label_val = {idx: label for label, idx in label2id_val.items()}\n",
    "\n",
    "label2id_test = {label: idx for idx, label in enumerate(labels_test)}\n",
    "id2label_test = {idx: label for label, idx in label2id_test.items()}\n",
    "\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "print(\"Label mapping:\", label2id)\n",
    "print(\"Train distribution:\", train_df[\"label\"].value_counts())\n",
    "print(\"Val distribution:\", val_df[\"label\"].value_counts())\n",
    "print(\"Test distribution:\", test_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebbb5a",
   "metadata": {},
   "source": [
    "### Emotion class distribution\n",
    "Quick sanity check plots the six emotion classes per split so we can reference class balance in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101edaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "distribution_df = pd.concat(\n",
    "    [\n",
    "        train_df.assign(split=\"train\"),\n",
    "        val_df.assign(split=\"val\"),\n",
    "        test_df.assign(split=\"test\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.countplot(\n",
    "    data=distribution_df,\n",
    "    x=\"label\",\n",
    "    hue=\"split\",\n",
    "    order=labels,\n",
    "    palette=\"Set2\",\n",
    ")\n",
    "ax.set_xlabel(\"Emotion label\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Emotion distribution per split\")\n",
    "ax.tick_params(axis=\"x\", rotation=30)\n",
    "plt.legend(title=\"Split\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789a61d",
   "metadata": {},
   "source": [
    "Initializes the tokenizer and defines an EmotionDataset wrapper that tokenizes text samples on the fly before instantiating dataset objects for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c59402",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 128):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            row[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoded.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label_id\"], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = EmotionDataset(train_df, tokenizer)\n",
    "val_dataset = EmotionDataset(val_df, tokenizer)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a48bef",
   "metadata": {},
   "source": [
    "Creates a classification configuration for DistilBERT and loads the base model with the updated label metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9657b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06f0d4",
   "metadata": {},
   "source": [
    "Defines compute_metrics so the Trainer can report accuracy and macro/weighted F1 scores during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeaf3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdb68f",
   "metadata": {},
   "source": [
    "### Trainer configuration\n",
    "Adjust `per_device_train_batch_size`, `num_train_epochs`, or `learning_rate` for your hardware budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85733489",
   "metadata": {},
   "source": [
    "Configures TrainingArguments (batch sizes, epochs, evaluation cadence, etc.) and instantiates the Hugging Face Trainer with datasets and metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c927ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ARTIFACT_ROOT / \"checkpoints\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41792f06",
   "metadata": {},
   "source": [
    "Optionally kicks off fine-tuning via trainer.train() and evaluates the pre-trained checkpoint to provide baseline metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3982ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch full fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Example: evaluate initial (pre-trained) checkpoint before fine-tuning\n",
    "pretrain_metrics = trainer.evaluate()\n",
    "print(pretrain_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8d317",
   "metadata": {},
   "source": [
    "Runs final evaluation on the test split, then saves the fine-tuned model, tokenizer, and label mapping artifacts for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ba7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After calling trainer.train(), run evaluation and save artifacts\n",
    "eval_metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test metrics:\", eval_metrics)\n",
    "\n",
    "trainer.save_model(ARTIFACT_ROOT / \"hf_model\")\n",
    "tokenizer.save_pretrained(ARTIFACT_ROOT / \"hf_model\")\n",
    "\n",
    "with open(ARTIFACT_ROOT / \"label2id.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(label2id, fp, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2fb46",
   "metadata": {},
   "source": [
    "### Training diagnostics\n",
    "Use the trainer log history to visualize how training and validation loss evolved across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb972ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "if history.empty:\n",
    "    print(\"Trainer log history is empty; run trainer.train() first.\")\n",
    "else:\n",
    "    train_history = history.dropna(subset=[\"loss\"])\n",
    "    eval_history = history.dropna(subset=[\"eval_loss\"])\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    if not train_history.empty:\n",
    "        ax.plot(train_history[\"step\"], train_history[\"loss\"], label=\"train loss\", marker=\"o\", linewidth=2, alpha=0.8)\n",
    "    if not eval_history.empty:\n",
    "        ax.plot(eval_history[\"step\"], eval_history[\"eval_loss\"], label=\"validation loss\", marker=\"o\", linewidth=2, alpha=0.8)\n",
    "    ax.set_xlabel(\"Training step\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training vs. validation loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    display_cols = [col for col in [\"epoch\", \"step\", \"loss\", \"eval_loss\", \"eval_accuracy\", \"eval_f1_macro\"] if col in history.columns]\n",
    "    if display_cols:\n",
    "        display(history[display_cols].dropna(how=\"all\").tail())\n",
    "    best_checkpoint = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
    "    best_metric = getattr(trainer.state, \"best_metric\", None) or getattr(trainer.state, \"best_model_metric\", None)\n",
    "    best_metric_name = (getattr(trainer.args, \"metric_for_best_model\", None)\n",
    "                        or getattr(trainer.state, \"best_model_metric_name\", None))\n",
    "    best_step = getattr(trainer.state, \"best_iteration\", None)\n",
    "    if best_step is None and best_checkpoint:\n",
    "        step_fragment = Path(best_checkpoint).name.split(\"-\")[-1]\n",
    "        if step_fragment.isdigit():\n",
    "            best_step = int(step_fragment)\n",
    "    best_epoch = None\n",
    "    if best_step is not None and \"epoch\" in history.columns:\n",
    "        epoch_series = history.loc[history[\"step\"] == best_step, \"epoch\"].dropna()\n",
    "        if not epoch_series.empty:\n",
    "            best_epoch = float(epoch_series.iloc[-1])\n",
    "    print(\"Best model checkpoint:\", best_checkpoint or \"not available yet\")\n",
    "    if best_metric is not None:\n",
    "        metric_label = best_metric_name or \"metric\"\n",
    "        print(f\"Best {metric_label}: {best_metric:.4f}\")\n",
    "    if best_step is not None:\n",
    "        if best_epoch is not None:\n",
    "            print(f\"Reached at step {best_step} (epoch {best_epoch:.2f})\")\n",
    "        else:\n",
    "            print(f\"Reached at step {best_step}\")\n",
    "    if display_cols and best_step is not None:\n",
    "        best_row = history.loc[history[\"step\"] == best_step, display_cols].dropna(how=\"all\")\n",
    "        if not best_row.empty:\n",
    "            print(\"Metrics at best checkpoint:\")\n",
    "            display(best_row.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02321df8",
   "metadata": {},
   "source": [
    "## Embedding helper\n",
    "Use the fine-tuned checkpoint to embed both prompts and lyrics via mean pooling of token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa61da7",
   "metadata": {},
   "source": [
    "Implements a lightweight TextEmotionEncoder wrapper that loads the fine-tuned model and exposes a mean-pooled encode() helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmotionEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_dir: Path, device: str | None = None):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModel\n",
    "\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_model = AutoModel.from_pretrained(model_dir).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch = texts[start:start+batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            outputs = self.base_model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            attention_mask = tokens.attention_mask.unsqueeze(-1)\n",
    "            summed = (token_embeddings * attention_mask).sum(dim=1)\n",
    "            counts = attention_mask.sum(dim=1)\n",
    "            mean_pooled = summed / counts\n",
    "            embeddings.append(mean_pooled.cpu().numpy())\n",
    "        return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f44614",
   "metadata": {},
   "source": [
    "Demonstrates encoding the training prompts with the helper and persists the resulting embeddings for downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (requires fine-tuned weights saved above)\n",
    "encoder = TextEmotionEncoder(ARTIFACT_ROOT / \"hf_model\")\n",
    "prompt_embeddings = encoder.encode(train_df[\"text\"].tolist())\n",
    "np.save(ARTIFACT_ROOT / \"train_prompt_embeddings.npy\", prompt_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850f64c",
   "metadata": {},
   "source": [
    "#### Quick demo\n",
    "Use a couple of sample prompts to verify the saved encoder produces fixed-size embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test for the text encoder\n",
    "if (ARTIFACT_ROOT / 'hf_model').exists():\n",
    "    demo_encoder = TextEmotionEncoder(ARTIFACT_ROOT / 'hf_model')\n",
    "    sample_prompts = [\n",
    "        'I feel hopeful and excited about tomorrow',\n",
    "    ]\n",
    "    demo_embeddings = demo_encoder.encode(sample_prompts)\n",
    "    print('Embeddings shape:', demo_embeddings.shape)\n",
    "    print('Sample embeddings:', demo_embeddings[0][2])\n",
    "else:\n",
    "    print('⚠️ Fine-tuned weights not found; run trainer.train() and save artifacts first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16310b36",
   "metadata": {},
   "source": [
    "### Embedding comparison visualizer\n",
    "Plot the most divergent embedding dimensions for two texts to highlight how their emotion vectors differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbe5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'encoder' not in globals():\n",
    "    if not (ARTIFACT_ROOT / 'hf_model').exists():\n",
    "        raise FileNotFoundError('Fine-tuned encoder not found; train and save the model first.')\n",
    "    encoder = TextEmotionEncoder(ARTIFACT_ROOT / 'hf_model')\n",
    "\n",
    "\n",
    "# def plot_embedding_comparison(text_a: str, text_b: str, labels = (\"Text A\", \"Text B\"), top_dims: int = 24) -> float:\n",
    "#     '''Plot the largest-magnitude component differences between two normalized embeddings.'''\n",
    "#     vectors = encoder.encode([text_a, text_b])\n",
    "#     vectors = vectors / np.clip(np.linalg.norm(vectors, axis=1, keepdims=True), a_min=1e-12, a_max=None)\n",
    "#     diffs = np.abs(vectors[0] - vectors[1])\n",
    "#     top_dims = min(top_dims, diffs.size)\n",
    "#     top_idx = np.argsort(diffs)[::-1][:top_dims]\n",
    "#     dims = [f'd{idx}' for idx in top_idx]\n",
    "#     x = np.arange(top_dims)\n",
    "#     width = 0.42\n",
    "#     fig, ax = plt.subplots(figsize=(14, 5))\n",
    "#     ax.bar(x - width / 2, vectors[0, top_idx], width, label=labels[0])\n",
    "#     ax.bar(x + width / 2, vectors[1, top_idx], width, label=labels[1])\n",
    "#     ax.set_xticks(x)\n",
    "#     ax.set_xticklabels(dims, rotation=45, ha='right')\n",
    "#     ax.set_ylabel('Normalized component value')\n",
    "#     cosine_sim = float(np.dot(vectors[0], vectors[1]))\n",
    "#     ax.set_title(f\"Embedding comparison (cosine similarity = {cosine_sim:.3f})\")\n",
    "#     ax.legend()\n",
    "#     ax.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     return cosine_sim\n",
    "# def plot_embedding_comparison(text_a: str, text_b: str, labels = (\"Text A\", \"Text B\"), top_dims: int = 24) -> float:\n",
    "#     '''Plot the largest-magnitude component differences between two normalized embeddings.'''\n",
    "#     vectors = encoder.encode([text_a, text_b])\n",
    "#     vectors = vectors / np.clip(np.linalg.norm(vectors, axis=1, keepdims=True), a_min=1e-12, a_max=None)\n",
    "#     diffs = np.abs(vectors[0] - vectors[1])\n",
    "#     top_dims = min(top_dims, diffs.size)\n",
    "#     top_idx = np.argsort(diffs)[::-1][:top_dims]\n",
    "#     dims = [f'd{idx}' for idx in top_idx]\n",
    "\n",
    "#     # spacing & layout tweaks to make axis easier to read\n",
    "#     spacing = 1.6                       # increase spacing between bars\n",
    "#     x = np.arange(top_dims) * spacing\n",
    "#     width = 0.6 * spacing               # bar width scales with spacing\n",
    "#     fig_w = max(14, top_dims * 0.6)     # expand figure width for many dims\n",
    "#     fig, ax = plt.subplots(figsize=(fig_w, 5))\n",
    "\n",
    "#     ax.bar(x - width / 2, vectors[0, top_idx], width, label=labels[0], color=\"#4C78A8\", alpha=0.95)\n",
    "#     ax.bar(x + width / 2, vectors[1, top_idx], width, label=labels[1], color=\"#F58518\", alpha=0.9)\n",
    "\n",
    "#     ax.set_xticks(x)\n",
    "#     ax.set_xticklabels(dims, rotation=45, ha='right')\n",
    "#     ax.set_xlim(x[0] - spacing * 0.5, x[-1] + spacing * 0.5)  # small margin at ends\n",
    "\n",
    "#     # reduce label clutter for large top_dims by hiding every other tick label\n",
    "#     # if top_dims > 12:\n",
    "#     #     for lbl in ax.xaxis.get_ticklabels()[::2]:\n",
    "#     #         lbl.set_visible(False)\n",
    "\n",
    "#     ax.set_ylabel('Normalized component value')\n",
    "#     cosine_sim = float(np.dot(vectors[0], vectors[1]))\n",
    "#     ax.set_title(f\"Embedding comparison (cosine similarity = {cosine_sim:.3f})\")\n",
    "#     ax.legend(frameon=False)\n",
    "#     ax.grid(True, axis='y', linestyle='--', alpha=0.25)\n",
    "#     plt.tight_layout()\n",
    "#     return cosine_sim\n",
    "\n",
    "# sample_text_a = \"I feel calm, supported, and ready to keep going.\"\n",
    "# sample_text_b = \"Everything feels chaotic and I'm running out of hope.\"\n",
    "# comparison_cos = plot_embedding_comparison(sample_text_a, sample_text_b, labels=(\"Calm + hopeful\", \"Anxious + hopeless\"))\n",
    "# print(f\"Cosine similarity: {comparison_cos:.3f}\")\n",
    "\n",
    "# sample_text_a = \"I feel calm, supported, and ready to keep going.\"\n",
    "# sample_text_b = \"This is so peaceful and reassuring, I feel at ease.\"\n",
    "# comparison_cos = plot_embedding_comparison(sample_text_a, sample_text_b, labels=(\"Calm + hopeful\", \"Peaceful + reassuring\"))\n",
    "# print(f\"Cosine similarity: {comparison_cos:.3f}\")\n",
    "\n",
    "def plot_embedding_comparison(text_a: str, text_b: str, labels=(\"Text A\", \"Text B\"), top_dims: int=24) -> float:\n",
    "    '''Plot the largest-magnitude component differences between two normalized embeddings.'''\n",
    "    vectors = encoder.encode([text_a, text_b])\n",
    "    vectors = vectors / np.clip(np.linalg.norm(vectors, axis=1, keepdims=True), a_min=1e-12, a_max=None)\n",
    "    diffs = np.abs(vectors[0] - vectors[1])\n",
    "    top_dims = min(top_dims, diffs.size)\n",
    "    top_idx = np.argsort(diffs)[::-1][:top_dims]\n",
    "    dims = [f'd{idx}' for idx in top_idx]\n",
    "\n",
    "    spacing = 1.6\n",
    "    x = np.arange(top_dims) * spacing\n",
    "    width = 0.6 * spacing\n",
    "    fig_w = max(14, top_dims * 0.6)\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, 5))\n",
    "\n",
    "    # Plot bars\n",
    "    ax.bar(x - width / 2, vectors[0, top_idx], width, label=labels[0], color=\"#4C78A8\", alpha=0.95)\n",
    "    ax.bar(x + width / 2, vectors[1, top_idx], width, label=labels[1], color=\"#F58518\", alpha=0.9)\n",
    "\n",
    "    # Add text above bars showing magnitude difference\n",
    "    for i, idx in enumerate(top_idx):\n",
    "        mag = diffs[idx]\n",
    "        y_pos = max(vectors[0, idx], vectors[1, idx]) + 0.01  # place text above tallest bar\n",
    "        ax.text(x[i], y_pos, f\"{mag:.3f}\", ha='center', va='bottom', fontsize=8, color='gray')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dims, rotation=45, ha='right')\n",
    "    ax.set_xlim(x[0] - spacing * 0.5, x[-1] + spacing * 0.5)\n",
    "    ax.set_ylabel('Normalized component value')\n",
    "\n",
    "    cosine_sim = float(np.dot(vectors[0], vectors[1]))\n",
    "    ax.set_title(f\"Embedding comparison (cosine similarity = {cosine_sim:.3f})\")\n",
    "    ax.legend(frameon=False)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sample_text_a = \"I feel calm, supported, and ready to keep going.\"\n",
    "sample_text_b = \"Everything feels chaotic and I'm running out of hope.\"\n",
    "comparison_cos = plot_embedding_comparison(sample_text_a, sample_text_b, labels=(\"Calm + hopeful\", \"Anxious + hopeless\"))\n",
    "print(f\"Cosine similarity: {comparison_cos:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e314991",
   "metadata": {},
   "source": [
    "## Build merged Spotify lyrics dataset\n",
    "Combine `songs.csv` and `spotify_songs.csv`, normalize key columns, and persist a cleaned dataset we can reuse for model training or lyric retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b47e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "SONGS_META_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"songs.csv\"\n",
    "SPOTIFY_SONGS_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"spotify_songs.csv\"\n",
    "COMBINED_SONGS_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"merged_spotify_songs.csv\"\n",
    "\n",
    "CANONICAL_COLUMNS = [\n",
    "    \"song_id\",\n",
    "    \"name\",\n",
    "    \"artists\",\n",
    "    \"album_name\",\n",
    "    \"album_release_date\",\n",
    "    \"playlist_genre\",\n",
    "    \"playlist_subgenre\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"duration_ms\",\n",
    "    \"track_popularity\",\n",
    "    \"lyrics\",\n",
    "    \"language\",\n",
    "    \"source\",\n",
    "]\n",
    "\n",
    "def _parse_lyrics(raw: str) -> str:\n",
    "    if pd.isna(raw):\n",
    "        return \"\"\n",
    "    text = str(raw).strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if text.startswith(\"[\") and text.endswith(\"]\"):\n",
    "        try:\n",
    "            tokens = ast.literal_eval(text)\n",
    "            if isinstance(tokens, (list, tuple)):\n",
    "                text = \" \".join(str(tok) for tok in tokens)\n",
    "        except Exception:\n",
    "            text = text.replace(\"[\", \" \").replace(\"]\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def _clean_artists(value: str) -> str:\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    value = str(value)\n",
    "    value = value.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    value = value.replace(\"'\", \"\")\n",
    "    value = re.sub(r\"\\s+\", \" \", value)\n",
    "    return value.strip()\n",
    "\n",
    "def _standardize_song_frame(df: pd.DataFrame, source: str) -> pd.DataFrame:\n",
    "    rename_map = {\n",
    "        \"track_id\": \"song_id\",\n",
    "        \"track_name\": \"name\",\n",
    "        \"track_artist\": \"artists\",\n",
    "        \"track_album_name\": \"album_name\",\n",
    "        \"track_album_release_date\": \"album_release_date\",\n",
    "        \"playlist_subgenre\": \"playlist_subgenre\",\n",
    "        \"language\": \"language\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "    keep_cols = [col for col in CANONICAL_COLUMNS if col in df.columns]\n",
    "    missing_cols = [col for col in CANONICAL_COLUMNS if col not in df.columns]\n",
    "    for col in missing_cols:\n",
    "        df[col] = np.nan\n",
    "    df = df[CANONICAL_COLUMNS].copy()\n",
    "    df[\"source\"] = source\n",
    "    df[\"lyrics\"] = df[\"lyrics\"].apply(_parse_lyrics)\n",
    "    df = df[df[\"lyrics\"].str.len() >= 30]\n",
    "    df[\"artists\"] = df[\"artists\"].apply(_clean_artists)\n",
    "    df[\"album_name\"] = df[\"album_name\"].fillna(\"\")\n",
    "    df[\"language\"] = df[\"language\"].fillna(\"unknown\")\n",
    "    numeric_cols = [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"key\",\n",
    "        \"loudness\",\n",
    "        \"mode\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "        \"duration_ms\",\n",
    "        \"track_popularity\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"song_id\", \"lyrics\"])\n",
    "    df[\"lyrics\"] = df[\"lyrics\"].str.lower()\n",
    "    df[\"lyrics\"] = df[\"lyrics\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    df[\"lyrics_length\"] = df[\"lyrics\"].str.len()\n",
    "    return df\n",
    "\n",
    "songs_base = pd.read_csv(SONGS_META_PATH)\n",
    "spotify_additional = pd.read_csv(SPOTIFY_SONGS_PATH)\n",
    "\n",
    "songs_base = _standardize_song_frame(songs_base, source=\"songs_csv\")\n",
    "spotify_additional = _standardize_song_frame(spotify_additional, source=\"spotify_songs_csv\")\n",
    "\n",
    "merged_songs = pd.concat([songs_base, spotify_additional], ignore_index=True)\n",
    "merged_songs = merged_songs.drop_duplicates(subset=[\"song_id\"])\n",
    "merged_songs = merged_songs.reset_index(drop=True)\n",
    "\n",
    "print(f\"Merged songs: {len(merged_songs):,} rows\")\n",
    "display(merged_songs.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7b52c",
   "metadata": {},
   "source": [
    "Persist the cleaned corpus for downstream training/reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76986bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_songs.to_csv(COMBINED_SONGS_PATH, index=False)\n",
    "print(f\"Saved merged dataset to {COMBINED_SONGS_PATH}\")\n",
    "merged_songs.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c96a82",
   "metadata": {},
   "source": [
    "## Lyrics emotion tagging & retrieval setup\n",
    "Sample ~1k songs with lyrics, clean the text, and prep artifacts so prompts and lyrics share the same embedding space for quick experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRICS_SAMPLE_SIZE = 1000\n",
    "SONG_DATA_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"merged_spotify_songs.csv\"\n",
    "LYRICS_ARTIFACT_DIR = ARTIFACT_ROOT / \"lyrics_retrieval\"\n",
    "LYRICS_ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_lyrics_sample(csv_path: Path, sample_size: int = 1000, chunk_size: int = 5000) -> pd.DataFrame:\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(csv_path)\n",
    "\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    reservoir: List[dict] = []\n",
    "    total_rows = 0\n",
    "    usecols = [\"song_id\", \"name\", \"album_name\", \"artists\", \"lyrics\"]\n",
    "\n",
    "    for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=chunk_size):\n",
    "        chunk = chunk.dropna(subset=[\"lyrics\"])\n",
    "        chunk[\"lyrics\"] = (\n",
    "            chunk[\"lyrics\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        chunk = chunk[chunk[\"lyrics\"].str.len() >= 30]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "\n",
    "        for row in chunk.itertuples(index=False):\n",
    "            total_rows += 1\n",
    "            row_dict = row._asdict()\n",
    "            if len(reservoir) < sample_size:\n",
    "                reservoir.append(row_dict)\n",
    "            else:\n",
    "                j = rng.integers(0, total_rows)\n",
    "                if j < sample_size:\n",
    "                    reservoir[j] = row_dict\n",
    "\n",
    "    if not reservoir:\n",
    "        raise ValueError(\"No lyric rows sampled; check the dataset path.\")\n",
    "\n",
    "    sampled = pd.DataFrame(reservoir).reset_index(drop=True)\n",
    "    sampled[\"lyrics_length\"] = sampled[\"lyrics\"].str.len()\n",
    "    print(f\"Collected {len(sampled)} songs from {total_rows} lyric rows.\")\n",
    "    return sampled\n",
    "\n",
    "lyrics_df = load_lyrics_sample(SONG_DATA_PATH, sample_size=LYRICS_SAMPLE_SIZE)\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec892d",
   "metadata": {},
   "source": [
    "Predict an emotion label for each lyric with the fine-tuned classifier, embed the lyrics with the shared encoder, and persist lightweight artifacts for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (ARTIFACT_ROOT / \"hf_model\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing fine-tuned checkpoint at artifacts/text_encoder/hf_model. Run trainer.train() and rerun the save cell first.\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ARTIFACT_ROOT / \"hf_model\"\n",
    ").to(device)\n",
    "emotion_classifier.eval()\n",
    "\n",
    "lyrics_encoder = TextEmotionEncoder(ARTIFACT_ROOT / \"hf_model\", device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_emotions(texts: List[str], batch_size: int = 32) -> List[str]:\n",
    "    labels = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        encodings = lyrics_encoder.tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        logits = emotion_classifier(**encodings).logits\n",
    "        labels.extend(logits.argmax(dim=-1).cpu().tolist())\n",
    "    return [id2label[idx] for idx in labels]\n",
    "\n",
    "lyrics_df[\"emotion_label\"] = predict_emotions(lyrics_df[\"lyrics\"].tolist())\n",
    "\n",
    "lyrics_embeddings = lyrics_encoder.encode(lyrics_df[\"lyrics\"].tolist(), batch_size=32)\n",
    "lyric_unit_embeddings = lyrics_embeddings / np.clip(\n",
    "    np.linalg.norm(lyrics_embeddings, axis=1, keepdims=True),\n",
    "    a_min=1e-12,\n",
    "    a_max=None,\n",
    ")\n",
    "\n",
    "np.save(LYRICS_ARTIFACT_DIR / \"lyrics_embeddings.npy\", lyrics_embeddings)\n",
    "\n",
    "metadata_cols = [\n",
    "    \"song_id\",\n",
    "    \"name\",\n",
    "    \"album_name\",\n",
    "    \"artists\",\n",
    "    \"emotion_label\",\n",
    "    \"lyrics\",\n",
    "    \"lyrics_length\",\n",
    "]\n",
    "lyrics_metadata_path = LYRICS_ARTIFACT_DIR / \"lyrics_metadata.jsonl\"\n",
    "lyrics_df[metadata_cols].to_json(\n",
    "    lyrics_metadata_path,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "\n",
    "print(f\"Saved lyric embeddings to {LYRICS_ARTIFACT_DIR / 'lyrics_embeddings.npy'}\")\n",
    "print(f\"Saved lyric metadata to {lyrics_metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c7718",
   "metadata": {},
   "source": [
    "Retrieve the top-N songs whose lyric embeddings are closest to a prompt embedding (cosine similarity in the shared space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_songs(prompt: str, top_k: int = 5) -> pd.DataFrame:\n",
    "    if lyrics_df.empty:\n",
    "        raise ValueError(\"lyrics_df is empty. Run the sampling cell first.\")\n",
    "\n",
    "    prompt_vec = lyrics_encoder.encode([prompt])[0]\n",
    "    prompt_vec = prompt_vec / max(np.linalg.norm(prompt_vec), 1e-12)\n",
    "    scores = lyric_unit_embeddings @ prompt_vec\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    results = lyrics_df.iloc[top_idx].copy()\n",
    "    results[\"similarity\"] = scores[top_idx]\n",
    "    return results[[\"name\", \"artists\", \"album_name\", \"emotion_label\", \"similarity\", \"lyrics\"]]\n",
    "\n",
    "demo_prompt = \"I just spilt water on my notes and ruined all my hardwork\"\n",
    "top_matches = retrieve_songs(demo_prompt, top_k=3)\n",
    "\n",
    "for rank, row in enumerate(top_matches.itertuples(index=False), start=1):\n",
    "    print(f\"{rank}. {row.name} — {row.artists} | {row.emotion_label} | score={row.similarity:.3f}\")\n",
    "    print(row.lyrics[:200].replace(\"\\n\", \" \") + \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79898d92",
   "metadata": {},
   "source": [
    "### Embedding space snapshot\n",
    "Project prompt + lyric embeddings into 2D (UMAP fallback to t-SNE) to illustrate shared space clusters by emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f91094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    from umap import UMAP as UMAPReducer\n",
    "except ImportError:\n",
    "    UMAPReducer = None\n",
    "\n",
    "if not (ARTIFACT_ROOT / \"hf_model\").exists():\n",
    "    raise FileNotFoundError(\"Run fine-tuning and save artifacts before plotting embeddings.\")\n",
    "\n",
    "encoder = TextEmotionEncoder(ARTIFACT_ROOT / \"hf_model\")\n",
    "\n",
    "prompt_sample = train_df.sample(n=min(600, len(train_df)), random_state=SEED).copy()\n",
    "prompt_sample[\"source\"] = \"prompt\"\n",
    "prompt_sample[\"emotion\"] = prompt_sample[\"label\"]\n",
    "prompt_emb = encoder.encode(prompt_sample[\"text\"].tolist())\n",
    "\n",
    "if \"lyrics_df\" in globals() and isinstance(lyrics_df, pd.DataFrame) and not lyrics_df.empty:\n",
    "    lyric_sample = lyrics_df.sample(n=min(600, len(lyrics_df)), random_state=SEED).copy()\n",
    "    lyric_sample[\"source\"] = \"lyric\"\n",
    "    lyric_sample[\"emotion\"] = lyric_sample.get(\"emotion_label\", \"unknown\")\n",
    "    lyric_emb = encoder.encode(lyric_sample[\"lyrics\"].tolist())\n",
    "    combined_df = pd.concat([prompt_sample, lyric_sample], ignore_index=True)\n",
    "    combined_emb = np.vstack([prompt_emb, lyric_emb])\n",
    "else:\n",
    "    combined_df = prompt_sample.copy()\n",
    "    combined_emb = prompt_emb\n",
    "\n",
    "if UMAPReducer is not None:\n",
    "    reducer_name = \"UMAP\"\n",
    "    reducer = UMAPReducer(random_state=SEED, n_neighbors=30, min_dist=0.1)\n",
    "else:\n",
    "    reducer_name = \"t-SNE\"\n",
    "    reducer = TSNE(random_state=SEED, perplexity=30, init=\"random\", learning_rate=\"auto\")\n",
    "\n",
    "embedding_2d = reducer.fit_transform(combined_emb)\n",
    "plot_df = combined_df.reset_index(drop=True)\n",
    "plot_df[\"x\"] = embedding_2d[:, 0]\n",
    "plot_df[\"y\"] = embedding_2d[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    hue=\"emotion\",\n",
    "    style=\"source\",\n",
    "    palette=\"tab10\",\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    ")\n",
    "plt.title(f\"{reducer_name} projection of shared text/lyric embeddings\")\n",
    "plt.xlabel(\"dim-1\")\n",
    "plt.ylabel(\"dim-2\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e4998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
