{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d5623d",
   "metadata": {},
   "source": [
    "# Prompt2Song – Contrastive Retrieval & FAISS Indexing\n",
    "\n",
    "Align prompt embeddings with fused song embeddings via contrastive learning and build a FAISS index for fast recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71fdd4",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Load prompt and song artifacts produced by earlier notebooks\n",
    "- Train lightweight projection heads using an InfoNCE-style objective with in-batch negatives\n",
    "- Build a FAISS index over fused song embeddings for low-latency retrieval\n",
    "- Provide an end-to-end query function that maps free-text prompts to recommended songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4dbee6",
   "metadata": {},
   "source": [
    "Imports JSON handling, math/random utilities, PyTorch/NumPy, FAISS (if available), and supporting typing helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f079946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import faiss  # type: ignore\n",
    "except ImportError:\n",
    "    faiss = None\n",
    "    print(\"⚠️ FAISS not installed. Install faiss-cpu or faiss-gpu before running indexing steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43efe3",
   "metadata": {},
   "source": [
    "Resolves project directories for text/fusion artifacts, ensures retrieval output folder exists, and warns if prerequisites are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e554dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "if (NOTEBOOK_DIR / \"datasets\").exists():\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "TEXT_MODEL_DIR = PROJECT_ROOT / \"artifacts\" / \"text_encoder\" / \"hf_model\"\n",
    "FUSION_DIR = PROJECT_ROOT / \"artifacts\" / \"fusion\"\n",
    "RETRIEVAL_DIR = PROJECT_ROOT / \"artifacts\" / \"retrieval\"\n",
    "RETRIEVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TEXT_MODEL_DIR.exists() or not FUSION_DIR.exists():\n",
    "    print(\"⚠️ Missing prerequisites. Run notebooks 01 and 02 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50630792",
   "metadata": {},
   "source": [
    "Defines the TextEmotionEncoder wrapper that can produce embeddings and emotion probabilities via the fine-tuned classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd263f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmotionEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_dir: Path, device: str | None = None):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_model = AutoModel.from_pretrained(model_dir).to(self.device)\n",
    "        self.classifier = AutoModelForSequenceClassification.from_pretrained(model_dir).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts: List[str], batch_size: int = 32, max_length: int = 256) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch = texts[start:start + batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            outputs = self.base_model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            attention_mask = tokens.attention_mask.unsqueeze(-1)\n",
    "            summed = (token_embeddings * attention_mask).sum(dim=1)\n",
    "            counts = attention_mask.sum(dim=1)\n",
    "            mean_pooled = summed / counts\n",
    "            embeddings.append(mean_pooled.cpu().numpy())\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_label(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        preds = []\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch = texts[start:start + batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            outputs = self.classifier(**tokens)\n",
    "            logits = outputs.logits\n",
    "            preds.append(logits.softmax(dim=-1).cpu().numpy())\n",
    "        return np.vstack(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb10f0",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23504918",
   "metadata": {},
   "source": [
    "Parses the emotion prompt dataset splits from disk and prints dataset size for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fceef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emotion_split(path: Path) -> List[str]:\n",
    "    df = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            text, label = line.strip().split(\";\")\n",
    "            df.append({\"text\": text.strip(), \"label\": label.strip()})\n",
    "    return df\n",
    "\n",
    "train_prompts = load_emotion_split(PROJECT_ROOT / \"datasets\" / \"emotions_NLP\" / \"train.txt\")\n",
    "val_prompts = load_emotion_split(PROJECT_ROOT / \"datasets\" / \"emotions_NLP\" / \"val.txt\")\n",
    "test_prompts = load_emotion_split(PROJECT_ROOT / \"datasets\" / \"emotions_NLP\" / \"test.txt\")\n",
    "print(f\"Loaded {len(train_prompts)} train prompts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6da5b5",
   "metadata": {},
   "source": [
    "Initializes the encoder, loads the saved label mapping, and reports available emotion labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TextEmotionEncoder(TEXT_MODEL_DIR)\n",
    "label2id_path = PROJECT_ROOT / \"artifacts\" / \"text_encoder\" / \"label2id.json\"\n",
    "if label2id_path.exists():\n",
    "    label2id = json.loads(label2id_path.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    # fallback: infer from classifier config\n",
    "    label2id = encoder.classifier.config.label2id\n",
    "\n",
    "id2label = {int(idx): label for label, idx in label2id.items()}\n",
    "label_names = sorted(label2id.keys())\n",
    "print(\"Labels:\", label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8909af",
   "metadata": {},
   "source": [
    "### Embed prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623b07e",
   "metadata": {},
   "source": [
    "Encodes prompts into arrays of texts, labels, and embeddings for each split and saves the prompt embeddings to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_arrays(records):\n",
    "    texts = [r[\"text\"] for r in records]\n",
    "    labels = [r[\"label\"] for r in records]\n",
    "    label_ids = [label2id[label] for label in labels]\n",
    "    embeddings = encoder.encode(texts)\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"labels\": np.array(label_ids, dtype=np.int64),\n",
    "        \"embeddings\": embeddings.astype(np.float32),\n",
    "    }\n",
    "\n",
    "prompt_arrays = {\n",
    "    \"train\": to_arrays(train_prompts),\n",
    "    \"val\": to_arrays(val_prompts),\n",
    "    \"test\": to_arrays(test_prompts),\n",
    "}\n",
    "\n",
    "np.save(RETRIEVAL_DIR / \"prompt_train_embeddings.npy\", prompt_arrays[\"train\"][\"embeddings\"])\n",
    "np.save(RETRIEVAL_DIR / \"prompt_val_embeddings.npy\", prompt_arrays[\"val\"][\"embeddings\"])\n",
    "np.save(RETRIEVAL_DIR / \"prompt_test_embeddings.npy\", prompt_arrays[\"test\"][\"embeddings\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12380d2d",
   "metadata": {},
   "source": [
    "### Load fused song embeddings and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28799f",
   "metadata": {},
   "source": [
    "Loads lyric and fused song embeddings plus metadata generated by the previous notebook, ensuring prerequisites exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_embeddings_path = FUSION_DIR / \"lyric_embeddings.npy\"\n",
    "fused_embeddings_path = FUSION_DIR / \"fused_song_embeddings.npy\"\n",
    "metadata_path = FUSION_DIR / \"song_metadata.json\"\n",
    "\n",
    "if not fused_embeddings_path.exists():\n",
    "    raise FileNotFoundError(\"Run 02_audio_encoder_and_fusion.ipynb to generate fused song embeddings.\")\n",
    "\n",
    "lyric_embeddings = np.load(lyric_embeddings_path)\n",
    "fused_embeddings = np.load(fused_embeddings_path)\n",
    "song_metadata = json.loads(metadata_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Fused embeddings shape:\", fused_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8dab5",
   "metadata": {},
   "source": [
    "### Predict song emotion distributions using the classifier head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f8b6d",
   "metadata": {},
   "source": [
    "Derives per-song emotion probabilities from lyrics using the classifier to produce label ids and store soft label scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_texts = [text for text in song_metadata[\"titles\"]]\n",
    "lyrics_texts = [text for text in song_metadata.get(\"lyrics\", [])]\n",
    "\n",
    "if not lyrics_texts:\n",
    "    # If lyrics were not stored in metadata, fall back to re-reading the dataset\n",
    "    import pandas as pd\n",
    "    songs_df = pd.read_csv(PROJECT_ROOT / \"datasets\" / \"song_features\" / \"songs_with_attributes_and_lyrics.csv\")\n",
    "    valid_mask = songs_df[\"lyrics\"].fillna(\"\").str.len() > 0\n",
    "    lyrics_texts = songs_df.loc[valid_mask, \"lyrics\"].astype(str).tolist()\n",
    "else:\n",
    "    lyrics_texts = [lx if isinstance(lx, str) else \"\" for lx in lyrics_texts]\n",
    "\n",
    "song_probs = encoder.predict_label(lyrics_texts, batch_size=16)\n",
    "song_label_ids = song_probs.argmax(axis=1)\n",
    "np.save(RETRIEVAL_DIR / \"song_label_probs.npy\", song_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1536e",
   "metadata": {},
   "source": [
    "### Contrastive projection heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f57001",
   "metadata": {},
   "source": [
    "Defines shared projection heads for prompts and songs along with the target embedding dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = fused_embeddings.shape[1]\n",
    "projection_dim = 256\n",
    "\n",
    "def build_projection_head() -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(embedding_dim, projection_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(projection_dim, projection_dim),\n",
    "    )\n",
    "\n",
    "prompt_projector = build_projection_head()\n",
    "song_projector = build_projection_head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae95e6",
   "metadata": {},
   "source": [
    "### Contrastive training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca98fdf",
   "metadata": {},
   "source": [
    "Groups prompt and song indices by label so we can sample matched pairs during contrastive training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b096492",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_by_label: Dict[int, List[int]] = {}\n",
    "for idx, label in enumerate(prompt_arrays[\"train\"][\"labels\"]):\n",
    "    prompt_by_label.setdefault(int(label), []).append(idx)\n",
    "\n",
    "song_by_label: Dict[int, List[int]] = {}\n",
    "for idx, label in enumerate(song_label_ids):\n",
    "    song_by_label.setdefault(int(label), []).append(idx)\n",
    "\n",
    "labels_available = sorted(set(prompt_by_label.keys()) & set(song_by_label.keys()))\n",
    "print(\"Labels with both prompts and songs:\", labels_available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df48e7",
   "metadata": {},
   "source": [
    "Moves projectors to the active device, sets up the optimizer, and defines helper functions for batch sampling and the InfoNCE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30dbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt_projector = prompt_projector.to(device)\n",
    "song_projector = song_projector.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(prompt_projector.parameters()) + list(song_projector.parameters()), lr=1e-3)\n",
    "\n",
    "def sample_batch(batch_size: int = 64):\n",
    "    prompt_embeds = []\n",
    "    song_embeds = []\n",
    "    for _ in range(batch_size):\n",
    "        label = random.choice(labels_available)\n",
    "        p_idx = random.choice(prompt_by_label[label])\n",
    "        s_idx = random.choice(song_by_label[label])\n",
    "        prompt_embeds.append(prompt_arrays[\"train\"][\"embeddings\"][p_idx])\n",
    "        song_embeds.append(fused_embeddings[s_idx])\n",
    "    prompt_batch = torch.from_numpy(np.stack(prompt_embeds)).float().to(device)\n",
    "    song_batch = torch.from_numpy(np.stack(song_embeds)).float().to(device)\n",
    "    return prompt_batch, song_batch\n",
    "\n",
    "\n",
    "def info_nce_loss(prompt_z: torch.Tensor, song_z: torch.Tensor, temperature: float = 0.07):\n",
    "    prompt_norm = F.normalize(prompt_z, dim=-1)\n",
    "    song_norm = F.normalize(song_z, dim=-1)\n",
    "    logits = prompt_norm @ song_norm.T\n",
    "    logits = logits / temperature\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bad34",
   "metadata": {},
   "source": [
    "Implements the contrastive training loop that repeatedly samples matched pairs and optimizes the projection heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(epochs: int = 10, steps_per_epoch: int = 100, batch_size: int = 64):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for step in range(steps_per_epoch):\n",
    "            prompt_batch, song_batch = sample_batch(batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            prompt_z = prompt_projector(prompt_batch)\n",
    "            song_z = song_projector(song_batch)\n",
    "            loss = info_nce_loss(prompt_z, song_z)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / steps_per_epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | loss={avg_loss:.4f}\")\n",
    "\n",
    "# Uncomment to train\n",
    "# train_contrastive(epochs=15, steps_per_epoch=200, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023c049",
   "metadata": {},
   "source": [
    "### Persist projection heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d9e43",
   "metadata": {},
   "source": [
    "Placeholders for saving the trained projection heads to disk once training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(prompt_projector.state_dict(), RETRIEVAL_DIR / \"prompt_projector.pt\")\n",
    "# torch.save(song_projector.state_dict(), RETRIEVAL_DIR / \"song_projector.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadba6a8",
   "metadata": {},
   "source": [
    "### Build FAISS index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad382f",
   "metadata": {},
   "source": [
    "Builds a normalized FAISS index of projected song embeddings and persists both the index and numpy cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ebce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retrieval_index():\n",
    "    if faiss is None:\n",
    "        raise ImportError(\"faiss is required for indexing\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        song_tensor = torch.from_numpy(fused_embeddings).float().to(device)\n",
    "        projected = song_projector(song_tensor).cpu().numpy()\n",
    "        projected = projected / np.linalg.norm(projected, axis=1, keepdims=True)\n",
    "\n",
    "    dim = projected.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(projected.astype(np.float32))\n",
    "    faiss.write_index(index, str(RETRIEVAL_DIR / \"faiss_song.index\"))\n",
    "    np.save(RETRIEVAL_DIR / \"projected_song_embeddings.npy\", projected.astype(np.float32))\n",
    "    print(\"Indexed\", index.ntotal, \"songs\")\n",
    "\n",
    "# build_retrieval_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d5ebe",
   "metadata": {},
   "source": [
    "### Prompt-to-song search helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e1072",
   "metadata": {},
   "source": [
    "Provides helpers to reload the FAISS index and run prompt-to-song retrieval returning top-k recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd148813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faiss_index(path: Path):\n",
    "    if faiss is None:\n",
    "        raise ImportError(\"Install faiss before using retrieval\")\n",
    "    return faiss.read_index(str(path))\n",
    "\n",
    "\n",
    "def recommend(prompt_text: str, top_k: int = 5):\n",
    "    if faiss is None:\n",
    "        raise ImportError(\"Install faiss before using retrieval\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prompt_embedding = encoder.encode([prompt_text])\n",
    "        prompt_tensor = torch.from_numpy(prompt_embedding).float().to(device)\n",
    "        projected_prompt = prompt_projector(prompt_tensor)\n",
    "        projected_prompt = F.normalize(projected_prompt, dim=-1)\n",
    "\n",
    "    index = load_faiss_index(RETRIEVAL_DIR / \"faiss_song.index\")\n",
    "    song_embeddings = np.load(RETRIEVAL_DIR / \"projected_song_embeddings.npy\")\n",
    "\n",
    "    query = projected_prompt.cpu().numpy().astype(np.float32)\n",
    "    scores, indices = index.search(query, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"song_id\": song_metadata[\"song_ids\"][idx],\n",
    "            \"title\": song_metadata[\"titles\"][idx],\n",
    "            \"artists\": song_metadata[\"artists\"][idx],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Example invocation after training/indexing\n",
    "# recommend(\"my dog passed\", top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811c327",
   "metadata": {},
   "source": [
    "### Validation ideas\n",
    "- Evaluate retrieval quality by checking if prompts whose dominant emotion is *sadness* retrieve sad songs.\n",
    "- Create qualitative plots comparing cosine similarity distributions for positive vs negative pairs.\n",
    "- Consider additional supervision such as valence/arousal regression if labels become available."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
