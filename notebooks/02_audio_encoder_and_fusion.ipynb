{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b1844d5",
   "metadata": {},
   "source": [
    "# Prompt2Song – Audio Encoder & Gated Fusion\n",
    "\n",
    "Train an audio feature projection network and a gated fusion module that blends lyric and acoustic embeddings into a unified song representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb40d1",
   "metadata": {},
   "source": [
    "## Goals\n",
    "- Embed song lyrics with the fine-tuned text encoder\n",
    "- Learn an audio feature encoder that maps acoustic descriptors into the same emotion space\n",
    "- Train a gated fusion module that balances lyric and audio signals per song\n",
    "- Persist reusable models, scalers, and fused song embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9538ad8",
   "metadata": {},
   "source": [
    "Imports filesystem helpers, numerical libraries, PyTorch modules, and scikit-learn utilities required for the audio pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24906dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install scikit-learn before running this notebook.\") from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f7967",
   "metadata": {},
   "source": [
    "Identifies dataset/artifact locations, prepares output directories, and warns if the text encoder artifacts are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef94ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: /Users/himanshu/Documents/Github/prompt2song/datasets/song_features/songs_with_attributes_and_lyrics.csv\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "if (NOTEBOOK_DIR / \"datasets\").exists():\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "DATASET_PATH = PROJECT_ROOT / \"datasets\" / \"song_features\" / \"songs_with_attributes_and_lyrics.csv\"\n",
    "TEXT_MODEL_DIR = PROJECT_ROOT / \"artifacts\" / \"text_encoder\" / \"hf_model\"\n",
    "FUSION_ARTIFACTS = PROJECT_ROOT / \"artifacts\" / \"fusion\"\n",
    "FUSION_ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not TEXT_MODEL_DIR.exists():\n",
    "    print(\"⚠️ Fine-tuned text encoder not found. Run 01_text_emotion_encoder.ipynb first.\")\n",
    "\n",
    "print(f\"Using dataset: {DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65826c",
   "metadata": {},
   "source": [
    "Defines the TextEmotionEncoder wrapper so we can reuse the fine-tuned text model for lyric embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c6cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmotionEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_dir: Path, device: str | None = None):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "        self.device = \"mps\" if torch.backends.mps.is_available() else \"cuda\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_dir).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts: List[str], batch_size: int = 32, max_length: int = 512) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        batch_indices = range(0, len(texts), batch_size)\n",
    "        for start in tqdm(batch_indices, desc='Encoding lyrics', leave=False):\n",
    "            batch = texts[start:start + batch_size]\n",
    "            tokens = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            outputs = self.base_model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            attention_mask = tokens.attention_mask.unsqueeze(-1)\n",
    "            summed = (token_embeddings * attention_mask).sum(dim=1)\n",
    "            counts = attention_mask.sum(dim=1)\n",
    "            mean_pooled = summed / counts\n",
    "            embeddings.append(mean_pooled.cpu().numpy())\n",
    "        return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4537f",
   "metadata": {},
   "source": [
    "Loads the song metadata CSV, fills missing lyrics, and prints coverage statistics for songs and lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb7a8ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'album_name', 'artists', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'lyrics']\n",
      "Total songs: 955320\n",
      "Songs with lyrics: 955307\n"
     ]
    }
   ],
   "source": [
    "songs_df = pd.read_csv(DATASET_PATH)\n",
    "print(songs_df.columns.tolist())\n",
    "print(\"Total songs:\", len(songs_df))\n",
    "\n",
    "songs_df[\"lyrics\"] = songs_df[\"lyrics\"].fillna(\"\").astype(str)\n",
    "non_empty = songs_df[songs_df[\"lyrics\"].str.len() > 0].copy()\n",
    "print(\"Songs with lyrics:\", len(non_empty))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02f0d2",
   "metadata": {},
   "source": [
    "### Embed lyrics with the fine-tuned text encoder\n",
    "This step can be time-consuming; cache the result for reuse across training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612193d",
   "metadata": {},
   "source": [
    "Runs the text encoder to embed every available lyric and stores the embeddings on disk for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "917ba2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding lyrics:   0%|          | 0/59707 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Uncomment when the text encoder artifacts are available\u001b[39;00m\n\u001b[32m      2\u001b[39m encoder = TextEmotionEncoder(TEXT_MODEL_DIR)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m lyric_embeddings = \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_empty\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlyrics\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m np.save(FUSION_ARTIFACTS / \u001b[33m\"\u001b[39m\u001b[33mlyric_embeddings.npy\u001b[39m\u001b[33m\"\u001b[39m, lyric_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/4AL3py12/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mTextEmotionEncoder.encode\u001b[39m\u001b[34m(self, texts, batch_size, max_length)\u001b[39m\n\u001b[32m     27\u001b[39m     counts = attention_mask.sum(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     28\u001b[39m     mean_pooled = summed / counts\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     embeddings.append(\u001b[43mmean_pooled\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy())\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embeddings)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Uncomment when the text encoder artifacts are available\n",
    "encoder = TextEmotionEncoder(TEXT_MODEL_DIR)\n",
    "lyric_embeddings = encoder.encode(non_empty[\"lyrics\"].tolist(), batch_size=16, max_length=512)\n",
    "np.save(FUSION_ARTIFACTS / \"lyric_embeddings.npy\", lyric_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da73eb33",
   "metadata": {},
   "source": [
    "Loads cached lyric embeddings from disk and asserts they align with the filtered songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If embeddings were cached previously, load them here\n",
    "lyric_embeddings = np.load(FUSION_ARTIFACTS / \"lyric_embeddings.npy\")\n",
    "assert lyric_embeddings.shape[0] == len(non_empty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fbc1c",
   "metadata": {},
   "source": [
    "### Prepare acoustic features\n",
    "Select the numerical columns that describe each song's audio profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dafe7",
   "metadata": {},
   "source": [
    "Specifies which acoustic feature columns to use, scales them with StandardScaler, persists scaler parameters, and logs the resulting shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b11a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACOUSTIC_FEATURES = [\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"loudness\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"mode\",\n",
    "    \"key\",\n",
    "    \"duration_ms\",\n",
    "]\n",
    "\n",
    "feature_df = non_empty[ACOUSTIC_FEATURES].copy()\n",
    "feature_df = feature_df.replace([np.inf, -np.inf], np.nan).fillna(feature_df.median())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(feature_df.values)\n",
    "\n",
    "np.save(FUSION_ARTIFACTS / \"audio_feature_scaler_mean.npy\", scaler.mean_)\n",
    "np.save(FUSION_ARTIFACTS / \"audio_feature_scaler_scale.npy\", scaler.scale_)\n",
    "feature_order_path = FUSION_ARTIFACTS / \"audio_feature_order.json\"\n",
    "feature_order_path.write_text(\"\n",
    "\".join(ACOUSTIC_FEATURES), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Scaled features shape:\", scaled_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf32f7",
   "metadata": {},
   "source": [
    "### Audio encoder dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588b930",
   "metadata": {},
   "source": [
    "Implements a Dataset that pairs acoustic feature vectors with lyric embeddings for supervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33008220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioToLyricDataset(Dataset):\n",
    "    def __init__(self, features: np.ndarray, targets: np.ndarray):\n",
    "        assert features.shape[0] == targets.shape[0]\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.targets = torch.from_numpy(targets).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# dataset = AudioToLyricDataset(scaled_features.astype(np.float32), lyric_embeddings.astype(np.float32))\n",
    "# train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e3902",
   "metadata": {},
   "source": [
    "### Neural modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9542b",
   "metadata": {},
   "source": [
    "Defines the audio encoder MLP and gated fusion module that will combine lyric and audio representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEmotionEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, embedding_dim: int, hidden_dims: Tuple[int, ...] = (256, 512)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=0.1))\n",
    "            prev_dim = hidden\n",
    "        layers.append(nn.Linear(prev_dim, embedding_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, lyric_emb: torch.Tensor, audio_emb: torch.Tensor):\n",
    "        concat = torch.cat([lyric_emb, audio_emb], dim=-1)\n",
    "        gate_logits = self.gate(concat)\n",
    "        gate = torch.sigmoid(gate_logits)\n",
    "        fused = gate * lyric_emb + (1.0 - gate) * audio_emb\n",
    "        return fused, gate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad27d5",
   "metadata": {},
   "source": [
    "### Training utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6371c7",
   "metadata": {},
   "source": [
    "Provides a training loop for the audio encoder that minimizes MSE between predicted and target lyric embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_audio_encoder(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    epochs: int = 20,\n",
    "    lr: float = 1e-3,\n",
    "    device: str | None = None,\n",
    "    checkpoint_path: Path | None = None,\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for features, targets in dataloader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features)\n",
    "            loss = criterion(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * features.size(0)\n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "        print(f\"[Audio] epoch={epoch+1} loss={avg_loss:.4f}\")\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved audio encoder to {checkpoint_path}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e553ff0",
   "metadata": {},
   "source": [
    "Provides a training loop for the gated fusion module that learns blend weights between lyric and audio embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gated_fusion(\n",
    "    fusion: GatedFusion,\n",
    "    lyric_embeddings: torch.Tensor,\n",
    "    audio_embeddings: torch.Tensor,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    batch_size: int = 128,\n",
    "    device: str | None = None,\n",
    "    checkpoint_path: Path | None = None,\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    fusion = fusion.to(device)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(lyric_embeddings, audio_embeddings)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(fusion.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        fusion.train()\n",
    "        epoch_loss = 0.0\n",
    "        for lyric_batch, audio_batch in dataloader:\n",
    "            lyric_batch = lyric_batch.to(device)\n",
    "            audio_batch = audio_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            fused, gate = fusion(lyric_batch, audio_batch)\n",
    "            loss = mse_loss(fused, lyric_batch) + 0.1 * mse_loss(gate, torch.ones_like(gate))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * lyric_batch.size(0)\n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "        print(f\"[Fusion] epoch={epoch+1} loss={avg_loss:.4f}\")\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        torch.save(fusion.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved fusion module to {checkpoint_path}\")\n",
    "\n",
    "    return fusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507b022",
   "metadata": {},
   "source": [
    "### Training workflow (execute after caching lyric embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e026b54",
   "metadata": {},
   "source": [
    "Outlines an end-to-end training script showing how to fit the audio encoder, train fusion, and save resulting artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example script:\n",
    "embedding_dim = lyric_embeddings.shape[1]\n",
    "dataset = AudioToLyricDataset(scaled_features.astype(np.float32), lyric_embeddings.astype(np.float32))\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "audio_encoder = AudioEmotionEncoder(input_dim=len(ACOUSTIC_FEATURES), embedding_dim=embedding_dim)\n",
    "audio_encoder = train_audio_encoder(\n",
    "    audio_encoder,\n",
    "    train_loader,\n",
    "    epochs=30,\n",
    "    lr=5e-4,\n",
    "    checkpoint_path=FUSION_ARTIFACTS / \"audio_encoder.pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_embeddings = audio_encoder(torch.from_numpy(scaled_features).float()).cpu()\n",
    "\n",
    "fusion_module = GatedFusion(embedding_dim=embedding_dim)\n",
    "fusion_module = train_gated_fusion(\n",
    "    fusion_module,\n",
    "    torch.from_numpy(lyric_embeddings).float(),\n",
    "    audio_embeddings,\n",
    "    epochs=15,\n",
    "    lr=1e-3,\n",
    "    checkpoint_path=FUSION_ARTIFACTS / \"gated_fusion.pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    fused_embeddings, gates = fusion_module(\n",
    "        torch.from_numpy(lyric_embeddings).float(),\n",
    "        audio_embeddings,\n",
    "    )\n",
    "fused_embeddings = fused_embeddings.cpu().numpy()\n",
    "np.save(FUSION_ARTIFACTS / \"fused_song_embeddings.npy\", fused_embeddings)\n",
    "np.save(FUSION_ARTIFACTS / \"fusion_gates.npy\", gates.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccc5a9",
   "metadata": {},
   "source": [
    "### Metadata export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea359a",
   "metadata": {},
   "source": [
    "Serializes song ids, titles, artists, and lyrics to JSON so retrieval components can access consistent metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"song_ids\": non_empty[\"id\"].tolist(),\n",
    "    \"titles\": non_empty[\"name\"].tolist(),\n",
    "    \"artists\": non_empty[\"artists\"].tolist(),\n",
    "    \"lyrics\": non_empty[\"lyrics\"].astype(str).tolist(),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(FUSION_ARTIFACTS / \"song_metadata.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(metadata, fp, indent=2)\n",
    "print(\"Saved song metadata for retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89008405",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Finish training the audio encoder and gated fusion module.\n",
    "- Confirm `fused_song_embeddings.npy` exists for the retrieval notebook.\n",
    "- Optionally analyse `fusion_gates.npy` to see modality balance per song."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4AL3py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
